{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl9pkl9szvF5"
      },
      "source": [
        "# Project 1 Neuronal networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB0l6trB3cjg"
      },
      "source": [
        "## 1.- MNIST and FMINST datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "60-EeB_t3aBI"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "TRfdBm-9yenT",
        "outputId": "3e7c5969-b51d-4faa-efe2-95f82a4fb90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /home/vm/anaconda3/lib/python3.11/site-packages (0.16.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (3.1.37)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from wandb) (4.25.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /home/vm/anaconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/vm/anaconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vm/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/vm/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vm/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/vm/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /home/vm/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malejogonzalez9\u001b[0m (\u001b[33mzurdito\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb # Weight & Biases library to monitor training and compare models\n",
        "wandb.login() # API key: 569de9861dbe18fe8888f13ca66e39d2b12934ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N9jUVLu33o2"
      },
      "source": [
        "To obtain the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UQntwL9Qz41o"
      },
      "outputs": [],
      "source": [
        "### Run this cell\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training  data\n",
        "trainsetMNIST = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "train_MNIST = torch.utils.data.DataLoader(trainsetMNIST, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testsetMNIST = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "test_MNIST = torch.utils.data.DataLoader(testsetMNIST, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-veIn4g74DcK"
      },
      "source": [
        "To obtain the FMINST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HAKDZ32K4uGl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download and load the training data\n",
        "trainsetFMNIST = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "train_FMNIST = torch.utils.data.DataLoader(trainsetFMNIST, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testsetFMNIST = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "test_FMNIST = torch.utils.data.DataLoader(testsetFMNIST, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkG8UcIcyenX",
        "outputId": "317c215d-c2ab-40ec-ba9b-bcde7c3a6770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "tensor(-1.) tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(train_MNIST)   #To iterate through the dataset\n",
        "\n",
        "images, labels = next(dataiter)\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "print(torch.min(images), torch.max(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_ixSC0yenX",
        "outputId": "eaa36144-a29b-475e-a172-31a000f004ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "tensor(-1.) tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(train_FMNIST)   #To iterate through the dataset\n",
        "\n",
        "images, labels = next(dataiter)\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "print(torch.min(images), torch.max(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJw4fAPyenY"
      },
      "source": [
        "We can see that our images are of size 28 x 28 = 784, so the input layer is of size 784. It is important to see that the range of the values are -1,1 so for the output layer we will use the tanh.\n",
        "\n",
        "We also need to split the training data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3G3fUJkZcJuS"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "valid_MNIST = copy.deepcopy(train_MNIST)  # Creates a copy of the object\n",
        "\n",
        "#We take the first 45k images for training\n",
        "train_MNIST.dataset.data = train_MNIST.dataset.data[:45000,:,:]\n",
        "\n",
        "#And the rest for validation\n",
        "valid_MNIST.dataset.data = valid_MNIST.dataset.data[45000:,:,:]\n",
        "\n",
        "valid_FMNIST = copy.deepcopy(train_FMNIST)  # Creates a copy of the object\n",
        "\n",
        "#We take the first 45k images for training\n",
        "train_FMNIST.dataset.data = train_FMNIST.dataset.data[:45000,:,:]\n",
        "\n",
        "#And the rest for validation\n",
        "valid_FMNIST.dataset.data = valid_FMNIST.dataset.data[45000:,:,:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvJ5xmovyenY"
      },
      "source": [
        "## 3 layers at both encoder/decoder:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jB8JSqIAyenY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class autoencoder3(nn.Module):\n",
        "    def __init__(self, projected_dimension):\n",
        "        super().__init__()\n",
        "        self.projected_dimension = projected_dimension\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.learning_rate = 0.001\n",
        "        self.name_train = f\"3_layer_with_PSNR_train_dropout{self.projected_dimension}\"\n",
        "        self.name_test = f\"3_layer_with_PSNR_test_dropout{self.projected_dimension}\"\n",
        "        self.project = \"project_1\"\n",
        "        self.architecture = \"linear_autoencoder\"\n",
        "        self.dataset = \"MNIST\"\n",
        "        self.input_image_size = 784\n",
        "        self.outputs = []\n",
        "        self.loss_during_training = []\n",
        "        self.valid_loss_during_training = []\n",
        "\n",
        "\n",
        "\n",
        "        # Building an linear encoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # 784 -> projected dimension\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.input_image_size, 300),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.2), # Dropout with a probability of 0.2\n",
        "            nn.Linear(300, 150),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.2),\n",
        "            nn.Linear(150, self.projected_dimension),\n",
        "        )\n",
        "\n",
        "        # Building an linear decoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # The Sigmoid activation function\n",
        "        # outputs the value between 0 and 1\n",
        "        # projected dimension -> 784\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.projected_dimension, 150),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.2),\n",
        "            nn.Linear(150, 300),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.2),\n",
        "            nn.Linear(300, self.input_image_size),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), self.learning_rate)\n",
        "\n",
        "    def forward(self, image):\n",
        "        encoded = self.encoder(image)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def psnr(self, img1, img2):  #  Peak signal-to-noise ratio (PSNR)\n",
        "        mse_criterio = nn.MSELoss()\n",
        "        mse = mse_criterio(img1, img2)\n",
        "        max_pixel = 1.0\n",
        "        psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
        "        return psnr\n",
        "\n",
        "    def do_training(self, epochs, train_data, valid_data):\n",
        "        self.train()\n",
        "\n",
        "\n",
        "        #self.optimizer = optim.SGD(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        wandb.init(  # Esto es para que se guarde en la plataforma de wandb y nos evitamos tener que hacerlo manualmente\n",
        "            # Set the project where this run will be logged\n",
        "            project=self.project,\n",
        "            # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "            name=self.name_train,\n",
        "            # Track hyperparameters and run metadata\n",
        "            config={\n",
        "                \"learning_rate\": self.learning_rate,\n",
        "                \"architecture\": self.architecture,\n",
        "                \"dataset\": self.dataset,\n",
        "                \"epochs\": epochs,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Aqui es donde empieza realmente el entrenamiento\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.\n",
        "            for img, _ in train_data:\n",
        "                self.optimizer.zero_grad()\n",
        "                img = img.reshape(-1, self.input_image_size)\n",
        "                reconstructed = model.forward(img)\n",
        "                loss_train = self.criterion(reconstructed, img)\n",
        "\n",
        "                running_loss += loss_train.item()\n",
        "                loss_train.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            self.loss_during_training.append(running_loss/len(train_data))\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # set model to evaluation mode\n",
        "                self.eval()\n",
        "\n",
        "                running_loss = 0.\n",
        "\n",
        "                for images,_ in valid_data:\n",
        "\n",
        "                    images = images.reshape(-1, self.input_image_size)\n",
        "\n",
        "                    out = self.forward(images)\n",
        "\n",
        "                    loss_valid = self.criterion(out,images)\n",
        "\n",
        "                    running_loss += loss_valid.item()\n",
        "                self.valid_loss_during_training.append(running_loss/len(valid_data))\n",
        "\n",
        "            # set model back to train mode\n",
        "            self.train()\n",
        "\n",
        "            print(\"Epoch: \", epoch + 1, \"Loss train: \", self.loss_during_training[-1], \", Loss valid: \", self.valid_loss_during_training[-1])\n",
        "            wandb.log({\"projected dimension\": projected_dimensions[run], \"Loss train\": self.loss_during_training[-1], \"Loss valid\": self.valid_loss_during_training[-1]})\n",
        "            self.outputs.append((projected_dimensions[run], epoch, img, reconstructed))\n",
        "\n",
        "        wandb.finish()\n",
        "\n",
        "    def eval_performance(self,data_test):\n",
        "\n",
        "      performance = 0\n",
        "\n",
        "      # Turn off gradients for validation, saves memory and computations\n",
        "      with torch.no_grad():\n",
        "\n",
        "          # set model to evaluation mode\n",
        "          self.eval()\n",
        "\n",
        "          for images,_ in data_test:\n",
        "\n",
        "              img = images.view(-1, self.input_image_size)\n",
        "              reconstructed = self.forward(img)\n",
        "              performance = self.psnr(reconstructed, img)\n",
        "\n",
        "\n",
        "          return performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XikycahyenZ"
      },
      "source": [
        "Definition of parameters of the model and Peak signal-to-noise ratio (PSNR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uanbXKp1yenZ"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610,
          "referenced_widgets": [
            "d9b0853f92d24113bf68d518972f1b6c",
            "cb6be34e62f645f1b7313da93f8fd8d0",
            "3bccf6c053d346bf820a70dfc3d637db",
            "38c65e28f87c4827a3fc3cf0d852436d",
            "d2ad6d17b57f4fae8a5fdb115a7bf62f",
            "9865ba6fdc854b82ba94abc6facd1ccd",
            "42aced4bc8844412bb7e532354fc3c16",
            "fddcba5c13b9407ca4662c91f538c87e"
          ]
        },
        "id": "RyClfxVSyena",
        "outputId": "faa06e3c-867f-433f-9803-24970b2490e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/mnt/c/Users/Alejo/OneDrive - Universidad Carlos III de Madrid/UC3M/Year 4/Segundo Cuatri/Neural Networks/projects/wandb/run-20240305_162242-93z9488n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zurdito/project_1/runs/93z9488n' target=\"_blank\">3_layer_with_PSNR_train_dropout15</a></strong> to <a href='https://wandb.ai/zurdito/project_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zurdito/project_1' target=\"_blank\">https://wandb.ai/zurdito/project_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zurdito/project_1/runs/93z9488n' target=\"_blank\">https://wandb.ai/zurdito/project_1/runs/93z9488n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "projected_dimensions = [15, 30, 50, 100]\n",
        "epochs = 15\n",
        "models = []\n",
        "for run in range(len(projected_dimensions)):\n",
        "    model = autoencoder3(projected_dimensions[run])  # To initialize the model with the projected dimension\n",
        "    model.do_training(epochs,train_MNIST,valid_MNIST)\n",
        "    models.append(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZAr3o3IZ2BT"
      },
      "source": [
        "Performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWTZop0naABf",
        "outputId": "b1ba4907-ef82-4cad-92ca-fc4448ae9584"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(12.5798)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval_performance(test_MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cv1gKgZyena"
      },
      "source": [
        "Visualization of reconstructed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8F-4C1Tyena"
      },
      "outputs": [],
      "source": [
        "for k in range(0,epochs,2):\n",
        "  plt.figure(figsize=(9,2))\n",
        "  plt.gray()\n",
        "  imgs = model.outputs[k][2].detach().numpy()\n",
        "  recon = model.outputs[k][3].detach().numpy()\n",
        "  for i, item in enumerate(imgs):\n",
        "    if i >=9: break\n",
        "    plt.subplot(2,9,i+1)\n",
        "    item = item.reshape(-1,28,28)\n",
        "\n",
        "    plt.imshow(item[0])\n",
        "  for i, item in enumerate(recon):\n",
        "    if i >=9: break\n",
        "    plt.subplot(2,9,i+1+9)\n",
        "    item = item.reshape(-1,28,28)\n",
        "\n",
        "    plt.imshow(item[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C2_w-bg87bw"
      },
      "outputs": [],
      "source": [
        "# Adding noise\n",
        "def add_noise(img, variance):\n",
        "    noise = np.random.normal(loc=0, scale=np.sqrt(variance), size=img.shape)\n",
        "    return img + noise\n",
        "img = train_MNIST.dataset.data[1,:,:]\n",
        "noised_img = add_noise(img, 1000000)\n",
        "plt.imshow(img.numpy().reshape([28,28]), cmap='Greys_r')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amomyRpG-yUb"
      },
      "outputs": [],
      "source": [
        "plt.imshow(noised_img.numpy().reshape([28,28]), cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foBVOGbKLr6A"
      },
      "outputs": [],
      "source": [
        "reconstructed = model.forward(img)\n",
        "plt.imshow(reconstructed.numpy().reshape([28,28]), cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4R72KGfyena"
      },
      "source": [
        "# 5 layers at both encoder/decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdVjO0MAyena"
      },
      "outputs": [],
      "source": [
        "class autoencoder5(nn.Module):\n",
        "    def __init__(self,projected_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        # Building an linear encoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # 784 -> projected dimension\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 550),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(550, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300,200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200,projected_dimension)\n",
        "        )\n",
        "\n",
        "        # Building an linear decoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # The Sigmoid activation function\n",
        "        # outputs the value between 0 and 1\n",
        "        # projected dimension -> 784\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(projected_dimension, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400,550),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(550,784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        encoded = self.encoder(image)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQdGLr-uyenb"
      },
      "outputs": [],
      "source": [
        "total_runs = 4 # To test each of the projected dimensions\n",
        "for run in range(total_runs):\n",
        "  model = autoencoder5(projected_dimensions[run]) # To initialize the model with the projected dimension\n",
        "  optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "  wandb.init( # Esto es para que se guarde en la plataforma de wandb y nos evitamos tener que hacerlo manualmente\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"project_1\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"5_layer_{run}\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"learning_rate\": 0.01,\n",
        "      \"architecture\": \"linear_autoencoder\",\n",
        "      \"dataset\": \"MNIST\",\n",
        "      \"epochs\": 10,\n",
        "      })\n",
        "\n",
        "    # Aqui es donde empieza realmente el entrenamiento\n",
        "  epochs = 10\n",
        "  outputs = []\n",
        "  for epoch in range(epochs):\n",
        "    for (img,_) in train_MNIST:\n",
        "      img = img.reshape(-1,784)\n",
        "      reconstructed = model.forward(img)\n",
        "      loss = -psnr(reconstructed,img)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1, \"Loss: \", loss.item())\n",
        "    wandb.log({\"projected dimension\" :projected_dimensions[run], \"loss\": loss})\n",
        "    outputs.append((projected_dimensions[run],epoch,img,reconstructed))\n",
        "\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrESsVANyenc"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnaB6rdAyenc"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38c65e28f87c4827a3fc3cf0d852436d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bccf6c053d346bf820a70dfc3d637db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42aced4bc8844412bb7e532354fc3c16",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fddcba5c13b9407ca4662c91f538c87e",
            "value": 1
          }
        },
        "42aced4bc8844412bb7e532354fc3c16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9865ba6fdc854b82ba94abc6facd1ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb6be34e62f645f1b7313da93f8fd8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ad6d17b57f4fae8a5fdb115a7bf62f",
            "placeholder": "​",
            "style": "IPY_MODEL_9865ba6fdc854b82ba94abc6facd1ccd",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "d2ad6d17b57f4fae8a5fdb115a7bf62f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b0853f92d24113bf68d518972f1b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb6be34e62f645f1b7313da93f8fd8d0",
              "IPY_MODEL_3bccf6c053d346bf820a70dfc3d637db"
            ],
            "layout": "IPY_MODEL_38c65e28f87c4827a3fc3cf0d852436d"
          }
        },
        "fddcba5c13b9407ca4662c91f538c87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
