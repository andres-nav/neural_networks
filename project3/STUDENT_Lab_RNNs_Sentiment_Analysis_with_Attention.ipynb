{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: RNNs for Sentiment Analysis (with Attention)\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "In this notebook your goal is to take your solution to the previous Lab (Sentiment Analysis with RNNs) and add a simple attention layer before the classifier. Let $\\mathbf{h}_0,\\ldots,\\mathbf{h}_\\ell$ be the RNN output states ($\\ell$ is the last state before feeding the garbage tokens):\n",
    "- We will use $\\mathbf{h}_\\ell$ as the query ($\\mathbf{q}=\\mathbf{h}_\\ell$)\n",
    "- $\\mathbf{h}_0,\\ldots,\\mathbf{h}_\\ell$ are the keys\n",
    "- Construct a two-layer MLP to construct the unnormalized weights $w_j=a(\\mathbf{q},\\mathbf{h}_j)$, $j=1,\\ldots,\\ell$. For the intermediate layer, use $tanh()$ activation.\n",
    "- Normalized weights are given by $\\mathbf{\\alpha}=\\text{Softmax}(w_1,\\ldots,w_\\ell)$.\n",
    "- Input to the classifier is\n",
    "\\begin{align}\n",
    "\\mathbf{c} = \\sum_{j=1}^{\\ell}\\alpha_j \\mathbf{h}_j\n",
    "\\end{align}\n",
    "\n",
    "Analyze if performance is improved and visualize the attention weights for a couple of cases.\n",
    "\n",
    "**Note:** While you do not have to code much more given the solution of the previous lab, it is tricky to code the attention layer efficiently. \n",
    "\n",
    "**Note 2:** Note we do NOT include in the attention module the states corresponding to processing the garbage token # to normalize lengths. It's important to keep this in mind. You can do this by masking the unnormalized weights $w_j$ if you compute then for the whole RNN output sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
