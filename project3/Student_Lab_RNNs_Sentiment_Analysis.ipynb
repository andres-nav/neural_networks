{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b850da1",
   "metadata": {},
   "source": [
    "# Lab: RNNs for Sentiment Analysis\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "In this notebook we will deploy RNNs to perform sentiment analysis in a real dataset of finantial news. The required preprocesing using standard NLP libraries will be given to you (you have a course on NLP in the second term to learn this part) and your goal will be to define the RNN model, train it and validate the results.\n",
    "\n",
    "\n",
    "We will use the [Finantial Phrase Bank](https://www.researchgate.net/profile/Pekka-Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip), which contains  near 5000 sentences from finantial news:\n",
    "\n",
    "\n",
    ">*This release of the financial phrase bank covers a collection of 4840 sentences. The selected collection of phrases was annotated by 16 people with adequate background knowledge on financial markets. Three of the annotators were researchers and the remaining 13 annotators were master’s students at Aalto University School of Business with majors primarily in finance, accounting, and economics.*\n",
    ">\n",
    ">*The objective of the phrase level annotation task was to classify each example sentence into a positive, negative or neutral category by considering only the information explicitly available in the given sentence. Since the study is focused only on financial and economic domains, the annotators were asked to consider the sentences from the view point of an investor only; i.e. whether the news may have positive, negative or neutral influence on the stock price. As a result, sentences which have a sentiment that is not relevant from an economic or financial perspective are considered neutral.*\n",
    "\n",
    "Lets load the database and preprocess it using [SpaCy](https://spacy.io/). This part is all given to you, so no need to worry much.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use it in colab, update spacy \n",
    "#!pip install --upgrade spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download language model\n",
    "\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Sentences.txt\", \"r\", encoding=\"ISO-8859-1\") as sentences:\n",
    "    lines = sentences.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126ead9",
   "metadata": {},
   "source": [
    "In lines, we have a list of sentences and an associted label, separated by the character `@`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ba50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeefa0",
   "metadata": {},
   "source": [
    "We split the sentence and we code with 0 neutral labels, with 1 negative labels, and with 2 positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [l.split('@')[0] for l in lines]\n",
    "opinions = [l.split('@')[1] for l in lines]\n",
    "\n",
    "\n",
    "def code_opinion(l):\n",
    "    \n",
    "    d = 0\n",
    "    \n",
    "    if (l=='negative\\n'):\n",
    "        \n",
    "        d = 1\n",
    "        \n",
    "    elif (l=='positive\\n'):\n",
    "        \n",
    "        d = 2\n",
    "        \n",
    "    return d\n",
    "\n",
    "labels = np.array([code_opinion(l) for l in opinions])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b424746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Phrase\":phrases,\n",
    "                  \"Opinion\":opinions})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020659ea",
   "metadata": {},
   "source": [
    "Lets plot the histogram of the labels ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "\n",
    "ax.hist(labels)\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(['Neutral','Negative','Positive'],rotation=45)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0a68b",
   "metadata": {},
   "source": [
    "To simplify the problem, we're fusing together *Neutral* and *Positive* labels. So label 1 is *Negative review* and label 0 is *Positive+Neutral*. Also, we separate between train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3245b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels_bin = labels.copy()\n",
    "\n",
    "labels_bin[labels_bin==2] = 0 \n",
    "\n",
    "idx_data = np.arange(0,len(df),1)\n",
    "\n",
    "# Separamos train de test\n",
    "idx_train, idx_test, y_train, y_test = train_test_split(idx_data, labels_bin, test_size=0.2, random_state=0)\n",
    "\n",
    "# Separamos train de val\n",
    "idx_train, idx_val, y_train, y_val = train_test_split(idx_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "acc_baseline_train = np.sum(y_train==0)/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4b376",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee5655",
   "metadata": {},
   "source": [
    "With the following code, we pre-process using Spacy each sentence by removing words that are punctuation marks. We also remove [stopping words](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) (words that are not very informative). As a result, we have the list `norm_docs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84793ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") # Language model\n",
    "\n",
    "\n",
    "docs = [nlp(c) for c in phrases]\n",
    "\n",
    "# We remove punctuation marks (.,;:?!...) and stopping words\n",
    "def normalize(doc):\n",
    "    \n",
    "    return [w for w in doc if w.has_vector and not w.is_punct and not w.is_stop]\n",
    "\n",
    "norm_docs = [normalize(d) for d in docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e7c75",
   "metadata": {},
   "source": [
    "Every item of `norm_docs` corresponds to each of the sentences in the dataset. Each item is itself a list of spacy tokens (basically words with different attributes). The most important attribute for us the the *word embedding vector*. For instance, lets print the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The first original sentence is: {phrases[0]}\\n')\n",
    "\n",
    "print(f'The first normalized sentence is: {norm_docs[0]}. The length of the document is {len(norm_docs[0])} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985b023",
   "metadata": {},
   "source": [
    "We can access to the word embedding of each token using the attribute `.vector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687aecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The word embedding associated with the token {norm_docs[0][0].text} is\\n')\n",
    "print(norm_docs[0][0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b4698",
   "metadata": {},
   "source": [
    "You can check that the word embedding dimension is 300 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The WE dimension is {norm_docs[0][0].vector.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff1de2",
   "metadata": {},
   "source": [
    "### Length normalization\n",
    "\n",
    "Note that in this problem we're working with sequences of text of different length. This is in principle not a problem for RNNs. However, there's one additional pre-processing step that we have to do. When we feed the RNN function **all sequences in the mini-batch must have the same length**. Then we will use the corresponding state to make the prediction.\n",
    "\n",
    "For simplicity, we are taking here a non-efficient solution. Instead of normalize lengths at a batch level, we are normalizing the whole dataset so all sequences have the same length. To do so, we're adding **junk tokens** to all sentences so they artificially are of the same length. However, we will keep in a list the real length of each data, since the sentiment analysis prediction will be made using the RNN state corresponding to the **last real token**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dcda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with the length for each sentence\n",
    "\n",
    "lengths = [len(d) for d in norm_docs]\n",
    "\n",
    "# Maximum length (in train!)\n",
    "max_l = np.max([lengths[d] for d in idx_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add as many junk tokens (#) as needed to equalize the length of each sentence to max_l\n",
    "\n",
    "garbage_token = nlp('#') # Junk token --> #\n",
    "\n",
    "# We normalize the dataset\n",
    "\n",
    "norm_docs_eq_length = [norm_docs[d]+[garbage_token]*(max_l-lengths[d]) for d in range(len(norm_docs))]\n",
    "\n",
    "# Train documents\n",
    "\n",
    "docs_train = [norm_docs_eq_length[d] for d in idx_train]\n",
    "\n",
    "len_train = [lengths[d] for d in idx_train]\n",
    "\n",
    "# Validation documents\n",
    "\n",
    "docs_val = [norm_docs_eq_length[d] for d in idx_val]\n",
    "\n",
    "len_val = [lengths[d] for d in idx_val]\n",
    "\n",
    "# documentos test\n",
    "\n",
    "docs_test = [norm_docs_eq_length[d] for d in idx_test]\n",
    "\n",
    "len_test = [lengths[d] for d in idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7827c",
   "metadata": {},
   "source": [
    "Recall that for each document, the sequence of word embeddings is obtained using the `.vector` attribute per token ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea418f47",
   "metadata": {},
   "source": [
    "### RNN classification\n",
    "\n",
    "Our goal is to implement an LSTM that takes as input the sequence of word embeddings and predicts the binary label. In the list `lengths` we have the length per document (before adding the junk tokens). The LSTM prediction **must be done using the LSTM state after processing the last real token**. That means we ignore the LSTM states after we start processing the junk tokens.\n",
    "\n",
    "\n",
    "A few comments:\n",
    "\n",
    "- The LSTM is trained using mini-batches. So the input is a tensor of dimension (batch_size, seq_length, max_l). You have to generate this tensor using the lists implemented before (`docs_train`, `docs_val`, `docs_test`).\n",
    "- You don't care about the LSTM initializaiton. Use the default one.\n",
    "- The LSTM provides the list of states upon processing the sequences of max_l word embeddings. For each sentence, you have to predict the label using the appropiate state (the one after processing the last real word). My advise is that you store in a list the decision states for each data in the minibatch and then concatenate them in a tensor using `torch.stack`.\n",
    "- We will use a single MLP layer to perform the prediction. Between the LSTM and the MLP, we include a dropout layer. \n",
    "- The network will take some time to train, so my advise is to save the network parameters after each epoch (in a separate file), so early stopping can be implemented without re-training.\n",
    "- Validate the dimension of the LSTM state.\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~olmos/BBVA/RNN.png' width=800 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29faf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc64b04",
   "metadata": {},
   "source": [
    ">**Exercise:** Complete the following code to construct the RNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,prob=0.5):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.rnn = #YOUR CODE HERE\n",
    "        \n",
    "        # last, fully-connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size) \n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
    "        \n",
    "        # Capa dropout \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "\n",
    "    def forward(self, x, lengths, h0=None):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        - x: Sequences of word embeddings. Dimensions (batch_size, max_l, word_embedding_size)\n",
    "        - lengths: The real length of each sequence, excluding the junk # tokens! You use this to know what\n",
    "          RNN state you should use to classify\n",
    "\n",
    "        '''\n",
    "        \n",
    "        batch_size = x.size(0) \n",
    "        seq_length = x.size(1) \n",
    "        \n",
    "        # Compute the RNN output (sequence of states for the whole input)\n",
    "        r_out, _ =  #YOUR CODE HERE\n",
    "        \n",
    "        # Now it comes the tricky part. You have to stack in a (batch_size, hidden_dim) tensor, the right state\n",
    "        # for each sequece. Namely, the state after processing the last real token (not the junk # ones)\n",
    "        # The function torch.stack() can be handy\n",
    "        \n",
    "        aux = torch.stack([r_out[[d],#YOUR  CODE HERE,:] for d in range(batch_size)]).reshape([-1,self.hidden_dim])\n",
    "        \n",
    "        # We classify using such tensor (don't forget the dropout!)\n",
    "        \n",
    "        output = self.logsoftmax(#YOUR  CODE HERE)\n",
    "    \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e16f1",
   "metadata": {},
   "source": [
    "Let's first illustrate how we can get the output of the (untrained) RNN network given our texts. For example, let's get the RNN output for the first three training texts. The first step is to get the **sequences of word embeddings** of each of them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [1,2,3]\n",
    "\n",
    "#[w.vector for w in docs_train[d]] generates the list of word vectors for the d-th document \n",
    "x_input = torch.Tensor([[w.vector for w in docs_train[d]] for d in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=1\n",
    "\n",
    "np.array([docs_train[d][w].vector for w in range(len_train[d])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = torch.Tensor([np.mean([docs_train[d][w].vector for w in range(len_train[d])],0) for d in idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ffc43",
   "metadata": {},
   "source": [
    "The shape of `x_input` should be (3,max_l,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bba6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b4f00",
   "metadata": {},
   "source": [
    "Let's instantiate the RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcdf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_RNN = RNN(300,2,20,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050a813",
   "metadata": {},
   "source": [
    "> **Exercise:** evaluate the RNN otuput for `x_input` and check that the ouput dimensions make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = my_RNN.forward(#YOUR CODE HERE,[lengths[d] for d in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab796e",
   "metadata": {},
   "source": [
    "> **Exercise:** Complete the following class, which inherits the previous one and it adds a training loop, an evaluation method, and functionalities to save the model every few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5786518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_with_train(RNN):\n",
    "    \n",
    "    def __init__(self,input_size, output_size, hidden_dim, n_layers,prob=0.0,batch_size=50,lr=0.0005,saved_files='./saved_models/RNN_sentiment_analysis'):\n",
    "        \n",
    "        super().__init__(input_size, output_size, hidden_dim, n_layers,prob)  \n",
    "        \n",
    "        self.lr = lr # Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr) # Optimizer\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()               \n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = [] \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.saved_files = saved_files\n",
    "        \n",
    "        \n",
    "    def predict_proba(self,docs,lengths,Y=None):\n",
    "        \n",
    "        '''\n",
    "        We use this method to get the output of the network given a set of documents. If label is provided,\n",
    "        we get accuracy.\n",
    "        \n",
    "        - docs: documents, each encoded as a list of spacy tokens. They are length-normalized.\n",
    "        - lengths: real length of each document.\n",
    "        - Y: labels\n",
    "        '''\n",
    "        accuracy = 0.0\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "        \n",
    "            x_input = torch.Tensor([[w.vector for w in d] for d in docs])\n",
    "        \n",
    "            logprobs = self.forward(x_input,#YOUR CODE HERE).detach().numpy()\n",
    "            \n",
    "            if(len(Y)>0):\n",
    "            \n",
    "                accuracy = np.sum(np.argmax(logprobs,1)==Y)/np.shape(Y)[0]\n",
    "            \n",
    "        return logprobs,accuracy\n",
    "            \n",
    "        \n",
    "    def fit(self,docs_train,docs_val,Y,Yval,len_train,len_val,epochs=100,print_every=5):\n",
    "        \n",
    "        '''\n",
    "        Método de entrenamiento.\n",
    "        \n",
    "        - docs_train, docs_val: training/validation documents  (each is a list of spacy tokens). \n",
    "          Normalized in length!\n",
    "        - len_train/len_val: real lengths\n",
    "        '''\n",
    "        \n",
    "        self.print_every = print_every\n",
    "        \n",
    "        self.epochs=epochs\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        self.num_train = len(docs_train) # Number of training points\n",
    "        \n",
    "        self.num_batchs = np.floor(self.num_train/self.batch_size) # Number of training batches\n",
    "        \n",
    "        self.num_val = len(docs_val) # Number of validation points\n",
    "        \n",
    "        self.num_batchs_val = np.floor(self.num_val/self.batch_size) # Numero of validation batches\n",
    "        \n",
    "        labels = torch.Tensor(Y).type(torch.LongTensor) # Training labels\n",
    "        \n",
    "        labelsval = torch.Tensor(Yval).type(torch.LongTensor) # Validation labels\n",
    "        \n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            self.train() # Activate dropout\n",
    "            \n",
    "            # Random data permutation\n",
    "            \n",
    "            idx = np.random.permutation(self.num_train)\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for i in range(int(self.num_batchs)):\n",
    "                        \n",
    "                self.optim.zero_grad()  \n",
    "\n",
    "                # Indices of the data entering the batch\n",
    "\n",
    "                idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                                    \n",
    "                # Just in case there are empty documents, we ignore them\n",
    "                idx_batch = [d for d in idx_batch if len_train[d]>0]\n",
    "                \n",
    "                # We code each document using the sequences of word embeddings\n",
    "                \n",
    "                x_input = torch.Tensor([[w.vector for w in docs_train[d]] for d in #YOUR  CODE HERE])\n",
    "                \n",
    "                # We compute the classifier output for every data\n",
    "\n",
    "                out = self.forward(x_input,[len_train[#YOUR  CODE HERE] for d in idx_batch])\n",
    "                \n",
    "                # Cost function and gradient descent step\n",
    "\n",
    "                loss = self.criterion(out,labels[idx_batch])\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
    "                \n",
    "                # SGD steps\n",
    "                \n",
    "                self.optim.step()\n",
    "                \n",
    "            self.loss_during_training.append(running_loss/self.num_batchs)\n",
    "            \n",
    "            # We save model parameters\n",
    "            \n",
    "            torch.save(self.state_dict(), self.saved_files+'_epoch_'+str(e)+'.pth')\n",
    "            \n",
    "            # We repeat for validation\n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                \n",
    "                # set model to evaluation mode\n",
    "                self.eval()\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "                idx = np.random.permutation(self.num_val)\n",
    "\n",
    "                running_loss = 0.\n",
    "                \n",
    "                for i in range(int(self.num_batchs_val)):\n",
    "                    \n",
    "                    idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size] \n",
    "                    \n",
    "                    # Just in case there are empty documents, we ignore them\n",
    "                    idx_batch = [d for d in idx_batch if len_val[d]>0]\n",
    "                    \n",
    "                    x_input = torch.Tensor(#YOUR  CODE HERE)\n",
    "\n",
    "                    out = self.forward(x_input,[len_val[d] for d in idx_batch])\n",
    "\n",
    "                    loss = self.criterion(out,labelsval[idx_batch])\n",
    "\n",
    "                    running_loss += loss.item() \n",
    "                    \n",
    "                self.valid_loss_during_training.append(running_loss/self.num_batchs_val)    \n",
    "                    \n",
    "                \n",
    "\n",
    "            if(e % self.print_every == 0): \n",
    "\n",
    "                print(f\"Training loss after {e} epochs: {self.loss_during_training[-1]}. Validation loss: {self.valid_loss_during_training[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee5f2f",
   "metadata": {},
   "source": [
    "> **Exercise:** Instantiate and train the class using a hidden state of 20 dimensions and dropout probability equal to 0.3. Train for 40 epochs (can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_RNN = RNN_with_train(#YOUR  CODE HERE)\n",
    "\n",
    "#Call train\n",
    "\n",
    "my_RNN.fit(docs_train,docs_val,torch.Tensor(y_train),torch.Tensor(y_val),len_train,len_val,epochs=40,print_every=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1eed96",
   "metadata": {},
   "source": [
    "> **Exercise:** Plot both validation and training loss. Recover the model parameters for the epoch that minimized the validation loss. To do so, note that at every epoch the model parameters are save in a certain file named  `RNN_sentiment_analysis_epoch_X.pth` (unless you changed the default value), where `X` is the epoch. The function to re-state the parameters of the network are  `state_dict = my_RNN.load_state_dict(torch.load('RNN_sentiment_analysis_epoch_X.pth'))\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the train/val loss\n",
    "\n",
    "#YOUR  CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a62f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering the best validation parameters\n",
    "\n",
    "idx_min = np.argsort(#YOUR  CODE HERE)\n",
    "\n",
    "my_RNN.load_state_dict(torch.load(my_RNN.saved_files+'_epoch_'+str(idx_min[0])+'.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10d557",
   "metadata": {},
   "source": [
    "> **Exercise:** Using the method `predict_proba`, compute the accuracy and class probabilities for the data in the test set. Note that the method returns log-probabilities that you have to exponentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dae5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs,acc = #YOUR  CODE HERE\n",
    "\n",
    "probs = np.exp(logprobs)  #YOUR CODE HERE\n",
    "\n",
    "print(f\"The test accuracy is {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64223e",
   "metadata": {},
   "source": [
    "The following code plots the ROC curve of the classifiers, and it computes the area under the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a495c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, recall, thresholds = metrics.roc_curve(y_test, probs[:,1], pos_label=1) \n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.plot(fpr,recall,lw=2.5,label='ROC curve LSTM')\n",
    "plt.legend(loc=7)\n",
    "plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('Recall (R)')\n",
    "plt.title('Curva ROC')\n",
    "plt.show()\n",
    "\n",
    "area_roc_LSTM = metrics.roc_auc_score(y_test, probs[:,1])\n",
    "\n",
    "print(f\"AUC ROC for LSTM is {area_roc_LSTM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801f1e7",
   "metadata": {},
   "source": [
    "If you do not implement the optimal part, just know that the proposed MLP solution AUC is typically less than 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b3727",
   "metadata": {},
   "source": [
    "### (OPTIONAL) \n",
    "\n",
    "Compare the accuracy/ROC with those achieved by the MLP classifier that predicts the label using the **mean of the word embeddings** (without junk tokens). Consider a three layer MLP with 10 and 5 hidden units respectively.\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~olmos/BBVA/MLP.png' width=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799362e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916d0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
